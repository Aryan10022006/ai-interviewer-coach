# ğŸ” Logging & Debugging Guide

## Overview
The AI Interview Prep Coach now includes comprehensive logging to track agent execution and debug issues.

---

## ğŸ“Š What Gets Logged

### 1. **Preparation Phase** (`run_preparation_phase()`)
Shows the sequential execution of initial analysis agents:

```
ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€
                    PREPARATION PHASE
ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€

ğŸ“Š Step 1: Running Profiler Agent...
   ğŸ“Š ProfilerAgent: Processing resume (1234 chars)...
   ğŸ“Š ProfilerAgent: Analyzing JD (567 chars)...
   ğŸ“Š ProfilerAgent: Calling Gemini Flash for analysis...
   ğŸ“Š ProfilerAgent: Received 856 chars response
   ğŸ“Š ProfilerAgent: Successfully parsed JSON
   âœ… Profiler: Found 5 matching skills, identified 3 areas to probe.

ğŸ” Step 2: Running Researcher Agent...
   ğŸ” ResearcherAgent: Researching 'Google'...
   ğŸ” ResearcherAgent: Searching Tavily for 'Google engineering culture interview process'...
   ğŸ” ResearcherAgent: Found 3 results
   ğŸ” ResearcherAgent: Synthesizing 2456 chars of data...
   ğŸ” ResearcherAgent: Calling Gemini Flash to synthesize...
   ğŸ” ResearcherAgent: Generated 234 char intel summary
   âœ… Researcher: Found 3 sources on Google's interview culture

ğŸ¯ Step 3: Running Strategy Agent...
   ğŸ¯ StrategyAgent: Planning with 5 matched skills...
   ğŸ¯ StrategyAgent: Considering 2 skill gaps...
   ğŸ¯ StrategyAgent: Calling Gemini Flash for strategy...
   ğŸ¯ StrategyAgent: Generated 345 char strategy
   ğŸ¯ StrategyAgent: Set persona to 'neutral'
   âœ… Strategy: Planned neutral interview approach

ğŸ¤ Step 4: Generating First Question...
   ğŸ­ InterviewerAgent: Stage=intro, Persona=neutral, Q#1
   ğŸ­ InterviewerAgent: 0 messages in history
   ğŸ­ InterviewerAgent: Generating question with GeminiWrapper...
   ğŸ­ InterviewerAgent: Generated question (123 chars)
   âœ… Interviewer: Asking intro question (#1) in neutral tone

âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
               PREPARATION COMPLETE
âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
```

### 2. **Answer Processing** (`process_user_answer()`)
Shows the evaluation loop for each user response:

```
============================================================
ğŸ’¬ PROCESSING ANSWER #2
============================================================

ğŸ“¹ Running Vision Coach...
   âœ… Body language appears confident with good eye contact

ğŸ¤” Running Critic Agent...
   ğŸ¤” CriticAgent: Evaluating 456 char answer...
   ğŸ¤” CriticAgent: Calling Gemini Flash for evaluation...
   ğŸ¤” CriticAgent: Successfully parsed evaluation JSON
   ğŸ¤” CriticAgent: Score=8/10, Sentiment=confident
   âœ… Critic: Scored 8/10 - confident tone detected
   Score: 8/10

ğŸ“ˆ Stage: TECHNICAL

ğŸ¯ Decision: INTERVIEW

ğŸ¤ Generating Next Question...
   ğŸ­ InterviewerAgent: Stage=technical, Persona=neutral, Q#3
   ğŸ­ InterviewerAgent: 4 messages in history
   ğŸ­ InterviewerAgent: Generating question with GeminiWrapper...
   ğŸ­ InterviewerAgent: Generated question (234 chars)
   âœ… Interviewer: Asking technical question (#3) in neutral tone
   Question: Can you explain the difference between a list and a tuple in Python...
============================================================
```

### 3. **Final Report Generation** (`ReportAgent`)
Shows comprehensive performance analysis:

```
ğŸ“Š Generating Final Report...
   ğŸ“Š ReportAgent: Generating final report...
   ğŸ“Š ReportAgent: Analyzed 5 answers, avg score=7.4/10
   ğŸ“Š ReportAgent: 12 messages in transcript
   ğŸ“Š ReportAgent: Calling Gemini Flash to generate report...
   ğŸ“Š ReportAgent: Generated 1234 char report
   âœ… Report generated
```

---

## ğŸ¯ Agent Identification

Each agent has a unique emoji identifier:

| Agent | Emoji | Purpose |
|-------|-------|---------|
| **ProfilerAgent** | ğŸ“Š | Resume vs JD analysis |
| **ResearcherAgent** | ğŸ” | Company intel gathering |
| **StrategyAgent** | ğŸ¯ | Interview strategy planning |
| **InterviewerAgent** | ğŸ­ | Question generation |
| **CriticAgent** | ğŸ¤” | Answer evaluation |
| **ReportAgent** | ğŸ“Š | Final report generation |
| **VisionCoachAgent** | ğŸ“¹ | Body language analysis |

---

## ğŸ› Debugging Common Issues

### Issue 1: GraphRecursionError
**What it means:** The interview loop ran too many times without terminating.

**How to debug:**
1. Check the terminal logs to see which agent keeps repeating
2. Look for the stage progression: `ğŸ“ˆ Stage: TECHNICAL`
3. Verify the decision logic: `ğŸ¯ Decision: INTERVIEW` or `ğŸ¯ Decision: END`

**Solution:** The preparation phase now runs agents directly (not via graph) to avoid loops. Interview phase has proper termination conditions.

### Issue 2: JSON Parse Failures
**Symptoms:** You see warnings like `âš ï¸ ProfilerAgent: JSON parse failed`

**How to debug:**
1. Check which agent is failing: `ğŸ“Š ProfilerAgent`, `ğŸ¤” CriticAgent`, etc.
2. The log shows the error message: `(Expecting value: line 1 column 1)`
3. All agents have fallback data to continue execution

**Solution:** Agents now include better prompt engineering and fallback mechanisms.

### Issue 3: API Key Issues
**Symptoms:** `âŒ Missing` next to API keys at startup

**How to debug:**
1. Check startup logs:
   ```
   ğŸ”‘ Google API Key: âœ… Found
   ğŸ”‘ Groq API Key: âœ… Found
   ```
2. Verify `.env` file contains valid keys
3. Check if keys are placeholder values like `your_api_key_here`

**Solution:** Update `.env` with valid API keys.

---

## ğŸ“ˆ Tracking Agent Execution Flow

### Sequential Execution (Preparation Phase)
```
Profile â†’ Research â†’ Strategy â†’ First Question
   â†“         â†“          â†“            â†“
  ğŸ“Š        ğŸ”         ğŸ¯           ğŸ­
```

### Loop Execution (Interview Phase)
```
User Answer â†’ Vision (optional) â†’ Critic â†’ Stage Check â†’ Next Question
     â†“              ğŸ“¹                ğŸ¤”         ğŸ“ˆ            ğŸ­
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    (Repeats until "END" decision)
```

### Termination Conditions
The interview ends when:
1. **Question Count â‰¥ 8:** Reached maximum questions
2. **Stage = "closing":** Completed all interview stages
3. **Average Score < 5:** Candidate struggling too much
4. **Average Score > 9:** Candidate excelling, no need to continue

---

## ğŸ”§ Enabling More Verbose Logging

To add even more detailed logs, edit agents.py:

```python
# Add to any agent's run() method:
print(f"   ğŸ” DEBUG: State keys = {list(state.keys())}")
print(f"   ğŸ” DEBUG: Question count = {state.get('question_count', 0)}")
print(f"   ğŸ” DEBUG: Current stage = {state.get('interview_stage', 'N/A')}")
```

---

## ğŸ“ Log Analysis Tips

### Finding Performance Bottlenecks
1. Look for large character counts: `Received 5000+ chars response`
2. Check Tavily search times: `Found 0 results` (fallback used)
3. Monitor question generation: `Generated question (500+ chars)` (may be too long)

### Verifying Agent Decisions
1. **Stage Progression:** Should go `intro â†’ technical â†’ behavioral â†’ closing`
2. **Score Trends:** Watch for `Score=X/10` to see candidate improvement
3. **Persona Changes:** Check if `Persona=challenging` when candidate struggles

### Understanding Failures
1. **Fallback Usage:** Any `âš ï¸` indicates fallback was used
2. **JSON Errors:** Shows exact parsing error for debugging
3. **Empty Answers:** `No answer to evaluate, skipping...` means user didn't respond

---

## ğŸš€ Running Tests with Logging

Use the test script to see logging in action:

```powershell
cd d:\Projects\AI_Interview_prep_coach
python test_logging.py
```

This will simulate an interview and show all agent logs.

---

## ğŸ“Š Performance Metrics

The logs track these key metrics:

| Metric | Where to Find | Purpose |
|--------|---------------|---------|
| **Character counts** | `Processing resume (1234 chars)` | Input size validation |
| **API response size** | `Received 856 chars response` | LLM output monitoring |
| **Question count** | `Q#3` | Interview progress |
| **Score progression** | `Score=8/10` | Candidate performance trend |
| **Skill matches** | `Found 5 matching skills` | Profile quality |
| **Research results** | `Found 3 sources` | Data availability |
| **Agent reasoning** | `âœ… Profiler: Found...` | High-level summary |

---

## ğŸ“ Best Practices

1. **Monitor the preparation phase** - It should complete in 4 steps without loops
2. **Check question counts** - Should not exceed 8-10 questions
3. **Verify stage progression** - Should move forward, not get stuck
4. **Watch for fallbacks** - Too many `âš ï¸` warnings indicate API issues
5. **Track average scores** - Should stabilize after 3-4 questions

---

## ğŸ› ï¸ Troubleshooting Commands

### View Real-Time Logs
```powershell
# Run Streamlit and watch terminal output
streamlit run app.py

# The logs appear in the terminal where you ran the command
```

### Test Single Agent
```python
from agents import profiler
from state import AgentState

state = {
    'resume_text': 'Sample resume...',
    'job_description': 'Sample JD...'
}

result = profiler.run(state)
# Check terminal for logs
```

### Save Logs to File
```powershell
# Redirect output to file
streamlit run app.py 2>&1 | Tee-Object -FilePath interview_logs.txt
```

---

## ğŸ“ Support

If you encounter issues not covered here:

1. Check the terminal logs for specific error messages
2. Look for `âš ï¸` or `âŒ` symbols indicating failures
3. Verify all agents complete their tasks: `âœ… Agent: Done`
4. Review the stage progression and decision flow
5. Check API key validity and quota limits

---

## ğŸ¯ Summary

The logging system provides:
- âœ… **Real-time visibility** into agent execution
- âœ… **Error tracking** with fallback mechanisms
- âœ… **Performance metrics** for optimization
- âœ… **Debug information** for troubleshooting
- âœ… **Progress indicators** for user feedback

All logs appear in the terminal where you run `streamlit run app.py`.
